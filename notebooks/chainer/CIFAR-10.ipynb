{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "\n",
    "from chainer.datasets import split_dataset_random, TupleDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Let's load CIFAR-10 dataset.\n",
    "\n",
    "### Download tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, save_dir = './datasets/'):    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(save_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        return file_path\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:    \n",
    "        with open(file_path, 'wb') as fd:\n",
    "            for chunk in r.iter_content(chunk_size=128):\n",
    "                fd.write(chunk)\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "        \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "file_path = maybe_download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar(file_path):\n",
    "    target_dir = os.path.dirname(file_path)\n",
    "\n",
    "    with tarfile.open(file_path) as tar:\n",
    "        for n in tar.getnames():\n",
    "            member = os.path.abspath(os.path.join(target_dir, n))\n",
    "            print(member)\n",
    "            \n",
    "            if not member.startswith(os.path.abspath(target_dir)):\n",
    "                sys.exit(1)\n",
    "\n",
    "        tar.extractall(path=target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py\n",
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py/data_batch_4\n",
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py/readme.html\n",
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py/test_batch\n",
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py/data_batch_3\n",
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py/batches.meta\n",
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py/data_batch_2\n",
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py/data_batch_5\n",
      "/tmp/notebooks/chainer/datasets/cifar-10-batches-py/data_batch_1\n"
     ]
    }
   ],
   "source": [
    "extract_tar(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpickel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_data_dir = \"/tmp/notebooks/chainer/datasets/cifar-10-batches-py\"\n",
    "\n",
    "def load_cifar10(data_dir):\n",
    "    ## Train data\n",
    "    train_data = np.empty((0, 3 * 32 * 32), dtype=np.float32)\n",
    "    train_labels = np.empty((0), dtype=np.int32)\n",
    "\n",
    "    for i in range(1,6):\n",
    "        file = os.path.join(pickled_data_dir, \"data_batch_{}\".format(i))\n",
    "        batch = unpickle(file)\n",
    "\n",
    "        train_data = np.vstack((train_data, batch[b'data']))\n",
    "        train_labels = np.hstack((train_labels, np.array(batch[b'labels'])))\n",
    "\n",
    "    ## Test data\n",
    "    file = os.path.join(pickled_data_dir, \"test_batch\")\n",
    "    batch = unpickle(file)\n",
    "\n",
    "    test_data = np.empty((0, 3 * 32 * 32), dtype=np.float32)\n",
    "    test_labels = np.empty((0), dtype=np.int32)\n",
    "    test_data = np.vstack((test_data, batch[b'data']))\n",
    "    test_labels = np.hstack((test_labels, np.array(batch[b'labels'])))\n",
    "    \n",
    "    ## Meta\n",
    "    file = os.path.join(pickled_data_dir, \"batches.meta\")\n",
    "    meta = unpickle(file)\n",
    "    label_names = meta[b'label_names']\n",
    "    \n",
    "    train_labels = train_labels.astype(np.int32)\n",
    "    test_labels = test_labels.astype(np.int32)\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, label_names = load_cifar10(pickled_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (50000, 3072)\n",
      "train_labels shape: (50000,)\n",
      "test_data shape: (10000, 3072)\n",
      "test_labels shape: (10000,)\n",
      "[b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck']\n",
      "\n",
      "------------------\n",
      "\n",
      "train_data type: float32\n",
      "train_labels type: int32\n",
      "test_data type: float32\n",
      "test_labels type: int32\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data shape: {}\".format(train_data.shape))\n",
    "print(\"train_labels shape: {}\".format(train_labels.shape))\n",
    "print(\"test_data shape: {}\".format(test_data.shape))\n",
    "print(\"test_labels shape: {}\".format(test_labels.shape))\n",
    "print(label_names)\n",
    "\n",
    "print(\"\\n------------------\\n\")\n",
    "print(\"train_data type: {}\".format(train_data.dtype))\n",
    "print(\"train_labels type: {}\".format(train_labels.dtype))\n",
    "print(\"test_data type: {}\".format(test_data.dtype))\n",
    "print(\"test_labels type: {}\".format(test_labels.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(vector):\n",
    "    image = np.moveaxis(vector.reshape(3, 32, 32), 0, -1)\n",
    "    plt.imshow(image.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TupleDataset(train_data, train_labels)\n",
    "valid = TupleDataset(test_data, test_labels)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batch_size)\n",
    "valid_iter = iterators.SerialIterator(valid, batch_size, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model - LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5LikeConvNet(Chain):\n",
    "    def __init__(self):\n",
    "        super(LeNet5LikeConvNet, self).__init__()\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(3, 6, ksize=5, stride=1)\n",
    "            self.conv2 = L.Convolution2D(6, 16, ksize=5, stride=1)\n",
    "            self.conv3 = L.Convolution2D(16, 120, ksize=5, stride=1)\n",
    "            self.fc4  = L.Linear(None, 84)\n",
    "            self.out = L.Linear(84, 10)\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        X = F.reshape(x, (-1, 3, 32, 32))\n",
    "        X = X / 255\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        X = self.normalize(x)\n",
    "\n",
    "        a1 = F.relu(self.conv1(X))                                 # shape: (m, 6, 28, 28)\n",
    "        a2 = F.max_pooling_2d(a1, 2, stride=2)        # shape: (m, 6, 14, 14)\n",
    "        a3 = F.relu(self.conv2(a2))                              # shape: (m, 16, 10, 10)\n",
    "        a4 = F.max_pooling_2d(a3, 2, stride=2)       # shape: (m, 16, 5, 5)\n",
    "        a5 = F.relu(self.conv3(a4))                              # shape: (m, 120, 1, 1)\n",
    "        a6 = F.relu(self.fc4(a5))                                  # shape: (m, 84)\n",
    "        out =self.out(a6)                                              # shape: (m, 10) \n",
    "        \n",
    "        if chainer.config.train:\n",
    "            return out\n",
    "\n",
    "        return F.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 type is <class 'numpy.ndarray'> and value is [ 59.  43.  50. ... 140.  84.  72.]\n",
      "y0 type is <class 'numpy.int32'> and value is 6\n",
      "out type is <class 'chainer.variable.Variable'> and value is variable([[ 0.0400871   0.04615976 -0.03882615 -0.02823405  0.00059602\n",
      "            0.0173551   0.01771353  0.03117665  0.1386121  -0.04671846]])\n",
      "out shape is (1, 10)\n"
     ]
    }
   ],
   "source": [
    "x0, y0 = train[0]\n",
    "model = LeNet5LikeConvNet()\n",
    "\n",
    "print(\"x0 type is {} and value is {}\".format(type(x0), x0))\n",
    "print(\"y0 type is {} and value is {}\".format(type(y0), y0))\n",
    "\n",
    "print(\"out type is {} and value is {}\".format(type(model(x0)),  model(x0)))\n",
    "print(\"out shape is {}\".format(model(x0).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model,  train_iter, valid_iter, max_epoch=10, gpu_id = -1):\n",
    "    if gpu_id >= 0:\n",
    "        model.to_gpu(gpu_id)\n",
    "\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "    \n",
    "    updater = training.updater.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "\n",
    "    trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='out/mnist')\n",
    "    \n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.snapshot(filename='snapshot_epoch-{.updater.epoch}'))\n",
    "    trainer.extend(extensions.snapshot_object(model.predictor, filename='model_epoch-{.updater.epoch}'))\n",
    "    trainer.extend(extensions.Evaluator(valid_iter, model, device=gpu_id))\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    trainer.extend(extensions.dump_graph('main/loss'))\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.74288     0.361973       2.12267               0.440071                  61.9117       \n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "net = LeNet5LikeConvNet()\n",
    "model = L.Classifier(net)\n",
    "\n",
    "train_iter.reset()\n",
    "valid_iter.reset()\n",
    "\n",
    "trainer = create_trainer(model, train_iter, valid_iter, max_epoch=1)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model\n",
    "\n",
    "Add some convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(Chain):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(3, 8, ksize=5)\n",
    "            self.batch_norm1 = L.BatchNormalization(8)\n",
    "\n",
    "            self.conv2 = L.Convolution2D(8, 16, ksize=5)\n",
    "            self.batch_norm2 = L.BatchNormalization(16)\n",
    "            \n",
    "            self.conv3 = L.Convolution2D(16, 32, ksize=3, pad=1)\n",
    "            self.batch_norm3 = L.BatchNormalization(32)\n",
    "            \n",
    "            self.conv4 = L.Convolution2D(32, 64, ksize=3)\n",
    "            self.batch_norm4 = L.BatchNormalization(64)\n",
    "            \n",
    "            self.conv5 = L.Convolution2D(64, 512, ksize=3)\n",
    "            self.batch_norm5 = L.BatchNormalization(512)\n",
    "\n",
    "            self.fc6  = L.Linear(None, 84)\n",
    "            self.fc7  = L.Linear(None, 84)            \n",
    "            self.out = L.Linear(84, 10)\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        X = F.reshape(x, (-1, 3, 32, 32))\n",
    "        X = X / 255\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        X = self.normalize(x)\n",
    "\n",
    "        h = F.relu(self.batch_norm1(self.conv1(X)))  # shape: (m, 8, 28, 28)\n",
    "        h = F.max_pooling_2d(h, 2, stride=2)             # shape: (m, 8, 14, 14)\n",
    "        \n",
    "        h = F.relu(self.batch_norm2(self.conv2(h)))  # shape: (m, 16, 10, 10)\n",
    "        h = F.max_pooling_2d(h, 2, stride=2)             # shape: (m, 16, 5, 5)\n",
    "        \n",
    "        h = F.relu(self.batch_norm3(self.conv3(h)))  # shape: (m, 32, 5, 5)\n",
    "        h = F.relu(self.batch_norm4(self.conv4(h)))  # shape: (m, 64, 3, 3) \n",
    "        h = F.relu(self.batch_norm5(self.conv5(h)))  # shape: (m, 512, 1, 1) \n",
    "        \n",
    "        h = F.dropout(F.relu(self.fc6(h)))              # shape: (m, 128)\n",
    "        h = F.dropout(F.relu(self.fc7(h)))              # shape: (m, 128)\n",
    "\n",
    "        out =self.out(h)                                            # shape: (m, 10) \n",
    "        \n",
    "        if chainer.config.train:\n",
    "            return out\n",
    "\n",
    "        return F.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.93145     0.26057        2.16844               0.389933                  99.3239       \n",
      "\u001b[J2           1.63025     0.396839       2.09605               0.472211                  184.949       \n",
      "\u001b[J3           1.50398     0.458834       2.0521                0.499901                  276.741       \n",
      "\u001b[J4           1.41086     0.503796       2.0244                0.534711                  366.19        \n",
      "\u001b[J5           1.34192     0.53155        2.01357               0.540843                  466.463       \n",
      "\u001b[J6           1.2771      0.557372       2.01231               0.549446                  559.836       \n",
      "\u001b[J7           1.22595     0.578465       1.98659               0.5714                    657.491       \n",
      "\u001b[J8           1.17604     0.596094       1.95853               0.58218                   739.887       \n",
      "\u001b[J9           1.1145      0.619645       1.94802               0.591179                  822.606       \n",
      "\u001b[J10          1.06205     0.637888       1.94261               0.589102                  924.447       \n",
      "\u001b[J11          1.02274     0.653345       1.92543               0.60532                   1012.86       \n",
      "\u001b[J12          0.965905    0.671915       1.91223               0.610759                  1114.63       \n",
      "\u001b[J13          0.920701    0.688239       1.90379               0.614419                  1220.82       \n",
      "\u001b[J14          0.868144    0.707472       1.90023               0.607892                  1317.88       \n",
      "\u001b[J15          0.814135    0.722626       1.91175               0.603343                  1412.14       \n",
      "\u001b[J16          0.775077    0.738401       1.88878               0.612737                  1500.24       \n",
      "\u001b[J17          0.733289    0.752418       1.8976                0.592959                  1584.62       \n",
      "\u001b[J18          0.685788    0.766624       1.89602               0.595728                  1669.2        \n",
      "\u001b[J19          0.642149    0.780108       1.87475               0.610562                  1747.96       \n",
      "\u001b[J20          0.596581    0.796775       1.87974               0.604529                  1828.14       \n",
      "\u001b[J21          0.563942    0.808524       1.89082               0.585839                  1908          \n",
      "\u001b[J22          0.537383    0.817568       1.88212               0.601562                  1988.06       \n",
      "\u001b[J23          0.494752    0.832301       1.8794                0.596519                  2073.64       \n",
      "\u001b[J24          0.488987    0.834435       1.86613               0.609177                  2169.98       \n",
      "\u001b[J25          0.433384    0.853161       1.87219               0.599585                  2279.78       \n",
      "\u001b[J26          0.415141    0.858456       1.86964               0.598497                  2361.44       \n",
      "\u001b[J27          0.411995    0.860417       1.87078               0.600178                  2441.5        \n",
      "\u001b[J28          0.388088    0.870604       1.86866               0.602156                  2521.36       \n",
      "\u001b[J29          0.341527    0.88533        1.87021               0.597013                  2600.99       \n",
      "\u001b[J30          0.351176    0.882792       1.86995               0.596519                  2685.77       \n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "net = ConvNet()\n",
    "model = L.Classifier(net)\n",
    "\n",
    "train_iter.reset()\n",
    "valid_iter.reset()\n",
    "\n",
    "trainer = create_trainer(model, train_iter, valid_iter, max_epoch=30)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve over-fitting\n",
    "\n",
    "The main/loss is gradually reducing, but the validation/main/loss is in plateau. Let's solve over-fitting.\n",
    "\n",
    "#### Weight decay\n",
    "\n",
    "You can introduce Weight decay easily using just the following code.\n",
    "\n",
    "\n",
    "```python\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(0.01))\n",
    "```\n",
    "\n",
    "Also, you can introduce Weight decay on fully connected layers as follows.\n",
    "\n",
    "\n",
    "```\n",
    "    net = YourConvNetModel()\n",
    "    net.fc6.W.update_rule.add_hook(chainer.optimizer.WeightDecay(0.01))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.80402     0.33342        2.08875               0.464992                  90.955        \n",
      "\u001b[J2           1.46783     0.47928        2.03105               0.5268                    183.008       \n",
      "\u001b[J3           1.33523     0.536478       2.00521               0.550534                  275.787       \n",
      "\u001b[J4           1.25297     0.570652       1.96927               0.585344                  369.986       \n",
      "\u001b[J5           1.19268     0.595069       1.95462               0.597508                  465.256       \n",
      "\u001b[J6           1.13964     0.616186       1.94514               0.607991                  561.709       \n",
      "\u001b[J7           1.0997      0.631933       1.92637               0.620352                  658.121       \n",
      "\u001b[J8           1.05472     0.645533       1.9172                0.61254                   754.86        \n",
      "\u001b[J9           1.0229      0.659887       1.91587               0.614715                  851.796       \n",
      "\u001b[J10          0.987573    0.672874       1.92521               0.594541                  948.001       \n",
      "\u001b[J11          0.962877    0.680389       1.89601               0.640032                  1043.84       \n",
      "\u001b[J12          0.939906    0.688879       1.8902                0.631131                  1142.74       \n",
      "\u001b[J13          0.911329    0.699828       1.89081               0.622033                  1240.43       \n",
      "\u001b[J14          0.887294    0.707532       1.88183               0.637757                  1335.73       \n",
      "\u001b[J15          0.863955    0.714934       1.88829               0.629252                  1436.04       \n",
      "\u001b[J16          0.838116    0.723157       1.89955               0.598892                  1532.27       \n",
      "\u001b[J17          0.826094    0.7289         1.86641               0.649921                  1636.48       \n",
      "\u001b[J18          0.800276    0.737412       1.85715               0.660403                  1733.53       \n",
      "\u001b[J19          0.776938    0.745553       1.86855               0.628659                  1829.99       \n",
      "\u001b[J20          0.760842    0.748981       1.88092               0.629945                  1926.33       \n",
      "\u001b[J21          0.737096    0.758352       1.85143               0.653877                  2022.12       \n",
      "\u001b[J22          0.717139    0.760577       1.8546                0.643394                  2117.55       \n",
      "\u001b[J23          0.702993    0.767803       1.84388               0.653877                  2213.17       \n",
      "\u001b[J24          0.691671    0.772716       1.872                 0.637955                  2316.69       \n",
      "\u001b[J25          0.664006    0.782349       1.83408               0.662381                  2423.01       \n",
      "\u001b[J26          0.650249    0.785766       1.87145               0.621242                  2520.99       \n",
      "\u001b[J27          0.633377    0.790425       1.85047               0.642009                  2615.94       \n",
      "\u001b[J28          0.612888    0.799612       1.85476               0.639537                  2712.75       \n",
      "\u001b[J29          0.594015    0.805986       1.83448               0.664062                  2811.87       \n",
      "\u001b[J30          0.570724    0.811899       1.82969               0.662085                  2916.66       \n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "net = ConvNet()\n",
    "model = L.Classifier(net)\n",
    "\n",
    "train_iter.reset()\n",
    "valid_iter.reset()\n",
    "\n",
    "trainer = create_trainer(model, train_iter, valid_iter, max_epoch=30)\n",
    "optimizer = trainer.updater.get_optimizer('main')\n",
    "#optimizer.add_hook(chainer.optimizer.WeightDecay(0.01))\n",
    "\n",
    "# Weight Decay for fully connected layers\n",
    "net.fc6.W.update_rule.add_hook(chainer.optimizer.WeightDecay(0.03))\n",
    "net.fc7.W.update_rule.add_hook(chainer.optimizer.WeightDecay(0.03))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideration\n",
    "\n",
    "It looks weight decay method successfully reduces over-fitting.\n",
    "\n",
    "\n",
    "###### accuracy after 30-epochs\n",
    "\n",
    "|   |  main/accuracy  | validation/main/accuracy  | \n",
    "|---|---|---|\n",
    "| w/o weight decay  | 0.882792  | 0.596519 |\n",
    "| w weight decay  | 0.811899  | 0.662085 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
