{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chainer: 4.1.0\n",
      "NumPy: 1.14.2\n",
      "CuPy: Not Available\n"
     ]
    }
   ],
   "source": [
    "chainer.print_runtime_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "\n",
    "from chainer.datasets import split_dataset_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "\n",
    "Load the dataset and check the size and data type of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.datasets import mnist\n",
    "\n",
    "train_valid, test = mnist.get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_valid: <class 'chainer.datasets.tuple_dataset.TupleDataset'>\n",
      "test: <class 'chainer.datasets.tuple_dataset.TupleDataset'>\n",
      "# of train_valid: 60000\n",
      "# of test: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"train_valid: %s\" % type(train_valid))\n",
    "print(\"test: %s\" % type(test))\n",
    "\n",
    "print(\"# of train_valid: %s\" % len(train_valid))\n",
    "print(\"# of test: %s\" % len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411768 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.9686275  0.49803925 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764707 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.882353   0.6745098  0.9921569  0.9490197  0.76470596 0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215688\n",
      " 0.9333334  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9843138  0.3647059  0.32156864\n",
      " 0.32156864 0.21960786 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.8588236  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.77647066 0.7137255\n",
      " 0.9686275  0.9450981  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960788 0.9921569\n",
      " 0.9921569  0.80392164 0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.9921569  0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.9921569  0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.9921569  0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.9450981\n",
      " 0.882353   0.627451   0.42352945 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.94117653 0.9921569\n",
      " 0.9921569  0.4666667  0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.9921569  0.9921569\n",
      " 0.5882353  0.10588236 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.98823535 0.9921569  0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647065 0.9921569  0.97647065 0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.9921569\n",
      " 0.9921569  0.8117648  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980393  0.9921569  0.9921569  0.9921569  0.9803922  0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411766 0.44705886 0.86666673 0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.78823537 0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352942  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.77647066 0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.8588236  0.9921569  0.9921569  0.9921569  0.9921569  0.76470596\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568629 0.6745098  0.8862746  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9568628  0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.9921569\n",
      " 0.9921569  0.9921569  0.8313726  0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x0, y0 = train_valid[0]\n",
    "print(x0)\n",
    "print(y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "\n",
    "Split dataset so that the ratio of train-set : validation-set : test-set becomes around 60% : 20% : 20%.\n",
    "\n",
    "Ref. https://www.youtube.com/watch?v=M3qpIzy4MQk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = split_dataset_random(train_valid, 50000, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training dataset:  50000\n",
      "# of validation dataset:  10000\n",
      "# of test dataset:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"# of training dataset: \",  len(train))\n",
    "print(\"# of validation dataset: \", len(valid))\n",
    "print(\"# of test dataset: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare iterators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batch_size)\n",
    "valid_iter = iterators.SerialIterator(valid, batch_size, repeat=False, shuffle=False)\n",
    "test_iter  = iterators.SerialIterator(test, batch_size, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Logistic Regression Model\n",
    "\n",
    "Getting familiar with Chainer, let's build a simple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(Chain):\n",
    "    \n",
    "    def __init__(self, n_out = 10):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.fc = L.Linear(None, n_out)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model,  train_iter, valid_iter, max_epoch=10, gpu_id = -1):\n",
    "    if gpu_id >= 0:\n",
    "        model.to_gpu(gpu_id)\n",
    "\n",
    "    optimizer = optimizers.SGD()\n",
    "    optimizer.setup(model)\n",
    "    \n",
    "    updater = training.updater.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "\n",
    "    trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='.out')\n",
    "    \n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.snapshot(filename='snapshot_epoch-{.updater.epoch}'))\n",
    "    trainer.extend(extensions.snapshot_object(model.predictor, filename='model_epoch-{.updater.epoch}'))\n",
    "    trainer.extend(extensions.Evaluator(valid_iter, model, device=gpu_id))\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    trainer.extend(extensions.dump_graph('main/loss'))\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = L.Classifier(model)\n",
    "gpu_id = -1\n",
    "\n",
    "trainer = create_trainer(model, train_iter, valid_iter, max_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.3489      0.704803       0.894117              0.825752                  1.63583       \n",
      "\u001b[J2           0.764448    0.840813       0.668779              0.852848                  3.72292       \n",
      "\u001b[J3           0.620556    0.857492       0.577383              0.865012                  5.74692       \n",
      "\u001b[J4           0.55185     0.867148       0.526616              0.870945                  7.7762        \n",
      "\u001b[J5           0.509772    0.873561       0.493186              0.875494                  9.82244       \n",
      "\u001b[J6           0.481613    0.878105       0.469602              0.878758                  11.8938       \n",
      "\u001b[J7           0.460492    0.882013       0.451425              0.881527                  14.1173       \n",
      "\u001b[J8           0.443764    0.884956       0.437176              0.883999                  16.5765       \n",
      "\u001b[J9           0.430723    0.887228       0.425203              0.887263                  18.5882       \n",
      "\u001b[J10          0.419608    0.888867       0.415581              0.888647                  20.6155       \n",
      "\u001b[J11          0.410795    0.890425       0.407248              0.890229                  22.5651       \n",
      "\u001b[J12          0.402564    0.892463       0.400049              0.892306                  24.599        \n",
      "\u001b[J13          0.395509    0.893642       0.393721              0.893987                  26.5984       \n",
      "\u001b[J14          0.389819    0.894732       0.388535              0.894581                  28.5631       \n",
      "\u001b[J15          0.384112    0.89602        0.383357              0.895965                  30.9328       \n",
      "\u001b[J16          0.379294    0.896875       0.378523              0.897547                  32.9857       \n",
      "\u001b[J17          0.374983    0.897778       0.374452              0.899229                  35.3569       \n",
      "\u001b[J18          0.370631    0.899057       0.37077               0.899525                  37.5287       \n",
      "\u001b[J19          0.367138    0.89972        0.367189              0.900613                  40.0307       \n",
      "\u001b[J20          0.363474    0.900396       0.36434               0.901602                  43.1036       \n",
      "\u001b[J21          0.360698    0.900675       0.361446              0.901998                  45.7917       \n",
      "\u001b[J22          0.357595    0.901462       0.3586                0.902393                  47.9647       \n",
      "\u001b[J23          0.354656    0.902314       0.356321              0.902888                  49.9608       \n",
      "\u001b[J24          0.352467    0.902764       0.353823              0.90358                   51.9704       \n",
      "\u001b[J25          0.349809    0.902913       0.351694              0.904272                  54.017        \n",
      "\u001b[J26          0.34781     0.903692       0.349143              0.904569                  56.0154       \n",
      "\u001b[J27          0.345365    0.904527       0.347153              0.905854                  58.3398       \n",
      "\u001b[J28          0.343336    0.904731       0.345289              0.906349                  60.8234       \n",
      "\u001b[J29          0.341684    0.905531       0.343503              0.905558                  62.9428       \n",
      "\u001b[J30          0.339871    0.905769       0.341934              0.906744                  65.1292       \n",
      "\u001b[J31          0.338033    0.90611        0.340229              0.906744                  67.8918       \n",
      "\u001b[J32          0.336318    0.906571       0.339268              0.907931                  69.8996       \n",
      "\u001b[J33          0.33483     0.906949       0.337373              0.907041                  72.0921       \n",
      "\u001b[J34          0.333334    0.907249       0.336156              0.908129                  74.5858       \n",
      "\u001b[J35          0.331796    0.907472       0.334829              0.908821                  77.1602       \n",
      "\u001b[J36          0.330392    0.907888       0.333587              0.908327                  79.8575       \n",
      "\u001b[J37          0.329905    0.907888       0.332383              0.90892                   82.072        \n",
      "\u001b[J38          0.327218    0.908494       0.331185              0.909316                  84.2989       \n",
      "\u001b[J39          0.32689     0.908548       0.330151              0.909217                  86.5769       \n",
      "\u001b[J40          0.325381    0.909075       0.328925              0.910008                  89.1722       \n",
      "\u001b[J41          0.324467    0.909087       0.327686              0.909711                  91.1921       \n",
      "\u001b[J42          0.322879    0.909787       0.327224              0.910008                  93.3633       \n",
      "\u001b[J43          0.322512    0.910056       0.326019              0.910502                  95.6238       \n",
      "\u001b[J44          0.321526    0.910186       0.324958              0.910898                  98.2878       \n",
      "\u001b[J45          0.319883    0.910965       0.324177              0.911491                  100.981       \n",
      "\u001b[J46          0.319405    0.911058       0.323508              0.911096                  103.347       \n",
      "\u001b[J47          0.318284    0.911345       0.322548              0.911986                  105.511       \n",
      "\u001b[J48          0.317243    0.911518       0.322017              0.91248                   107.64        \n",
      "\u001b[J49          0.316626    0.911585       0.320863              0.911788                  109.702       \n",
      "\u001b[J50          0.31568     0.911885       0.320307              0.912085                  111.721       \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Logistic Regression with test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9152492088607594\n"
     ]
    }
   ],
   "source": [
    "test_evaluator = extensions.Evaluator(test_iter, model, device=gpu_id)\n",
    "results = test_evaluator()\n",
    "print('Test accuracy:', results['main/accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulid Multi Layer Perceptron\n",
    "\n",
    "Let's buld Multi Layer Perceptron model to achive more than 91% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "\n",
    "    def __init__(self, n_mid_units=100, n_out=10):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_mid_units)\n",
    "            self.l2 = L.Linear(None, n_mid_units)\n",
    "            self.l3 = L.Linear(None, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.63375     0.618147       0.888559              0.821005                  2.78392       \n",
      "\u001b[J2           0.644295    0.852841       0.505771              0.873022                  6.90437       \n",
      "\u001b[J3           0.447941    0.882512       0.40837               0.889537                  10.1411       \n",
      "\u001b[J4           0.38294     0.89536        0.365818              0.896559                  13.7691       \n",
      "\u001b[J5           0.348004    0.902574       0.337995              0.904866                  17.8336       \n",
      "\u001b[J6           0.325455    0.907612       0.318763              0.910502                  21.9345       \n",
      "\u001b[J7           0.307869    0.912004       0.305648              0.913469                  25.5184       \n",
      "\u001b[J8           0.294255    0.916266       0.291981              0.918216                  28.7364       \n",
      "\u001b[J9           0.282002    0.918858       0.282055              0.921282                  31.9949       \n",
      "\u001b[J10          0.271551    0.922754       0.271783              0.922963                  35.3039       \n",
      "\u001b[J11          0.262167    0.925481       0.262847              0.92504                   39.3859       \n",
      "\u001b[J12          0.253625    0.927809       0.254973              0.927116                  43.6002       \n",
      "\u001b[J13          0.245474    0.930607       0.249105              0.927907                  47.1432       \n",
      "\u001b[J14          0.238292    0.932552       0.240623              0.930578                  50.5762       \n",
      "\u001b[J15          0.231168    0.935042       0.237178              0.93216                   53.9346       \n",
      "\u001b[J16          0.225042    0.936619       0.229972              0.933248                  57.3384       \n",
      "\u001b[J17          0.218369    0.938439       0.225687              0.934237                  60.6209       \n",
      "\u001b[J18          0.212981    0.940078       0.218987              0.935819                  63.9308       \n",
      "\u001b[J19          0.207147    0.942187       0.214111              0.938192                  67.2013       \n",
      "\u001b[J20          0.202265    0.943874       0.209014              0.938786                  70.4722       \n",
      "\u001b[J21          0.197077    0.945153       0.205514              0.939379                  73.8179       \n",
      "\u001b[J22          0.192092    0.946695       0.199814              0.942148                  79.1566       \n",
      "\u001b[J23          0.187393    0.947051       0.197728              0.941752                  82.9855       \n",
      "\u001b[J24          0.182838    0.948498       0.191993              0.94462                   86.5958       \n",
      "\u001b[J25          0.178377    0.949828       0.188086              0.94462                   90.5376       \n",
      "\u001b[J26          0.17433     0.951267       0.185778              0.94729                   94.8489       \n",
      "\u001b[J27          0.170316    0.952183       0.182338              0.947191                  98.2771       \n",
      "\u001b[J28          0.166294    0.953465       0.177956              0.948279                  101.935       \n",
      "\u001b[J29          0.162854    0.954544       0.177183              0.948774                  105.565       \n",
      "\u001b[J30          0.159224    0.955288       0.172733              0.950356                  110.837       \n",
      "\u001b[J31          0.155908    0.956821       0.171547              0.951246                  114.444       \n",
      "\u001b[J32          0.15262     0.957452       0.167155              0.951938                  118.366       \n",
      "\u001b[J33          0.149552    0.9581         0.164401              0.952927                  122.597       \n",
      "\u001b[J34          0.146526    0.959299       0.162201              0.954114                  126.438       \n",
      "\u001b[J35          0.143598    0.959675       0.161148              0.953422                  130.82        \n",
      "\u001b[J36          0.14068     0.960778       0.157631              0.954608                  134.493       \n",
      "\u001b[J37          0.138432    0.961757       0.154858              0.955696                  138.325       \n",
      "\u001b[J38          0.13547     0.96236        0.153407              0.95629                   141.874       \n",
      "\u001b[J39          0.132907    0.963215       0.1512                0.956784                  146.522       \n",
      "\u001b[J40          0.130571    0.963462       0.148989              0.956586                  151.224       \n",
      "\u001b[J41          0.128286    0.964274       0.14774               0.957377                  155.463       \n",
      "\u001b[J42          0.126014    0.965693       0.146785              0.958366                  159.257       \n",
      "\u001b[J43          0.123911    0.965705       0.14446               0.958861                  163.375       \n",
      "\u001b[J44          0.121578    0.966832       0.142497              0.95807                   167.211       \n",
      "\u001b[J45          0.119714    0.967072       0.140927              0.959157                  170.971       \n",
      "\u001b[J46          0.117479    0.967448       0.139066              0.959157                  174.447       \n",
      "\u001b[J47          0.115546    0.96849        0.138908              0.959751                  178.36        \n",
      "\u001b[J48          0.113799    0.96869        0.13627               0.961135                  183.318       \n",
      "\u001b[J49          0.111946    0.969629       0.133849              0.961135                  187.631       \n",
      "\u001b[J50          0.110211    0.970189       0.133204              0.961729                  192.206       \n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "model = L.Classifier(model)\n",
    "\n",
    "train_iter.reset()\n",
    "valid_iter.reset()\n",
    "test_iter.reset()\n",
    "\n",
    "trainer = create_trainer(model, train_iter, valid_iter, max_epoch=50)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate MLP with test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9636075949367089\n"
     ]
    }
   ],
   "source": [
    "gpu_id = -1\n",
    "\n",
    "test_evaluator = extensions.Evaluator(test_iter, model, device=gpu_id)\n",
    "results = test_evaluator()\n",
    "print('Test accuracy:', results['main/accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Chainer Guides](https://docs.chainer.org/en/stable/guides/index.html)\n",
    "- [Chainer v4 ビギナー向けチュートリアル](https://qiita.com/mitmul/items/1e35fba085eb07a92560#%E5%AD%A6%E7%BF%92%E3%83%AB%E3%83%BC%E3%83%97%E3%82%92%E6%9B%B8%E3%81%84%E3%81%A6%E3%81%BF%E3%82%88%E3%81%86)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
