{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chainer: 4.1.0\n",
      "NumPy: 1.14.2\n",
      "CuPy: Not Available\n"
     ]
    }
   ],
   "source": [
    "chainer.print_runtime_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "\n",
    "from chainer.datasets import split_dataset_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "\n",
    "Load the dataset and check the size and data type of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.datasets import mnist\n",
    "\n",
    "train_valid, test = mnist.get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_valid: <class 'chainer.datasets.tuple_dataset.TupleDataset'>\n",
      "test: <class 'chainer.datasets.tuple_dataset.TupleDataset'>\n",
      "# of train_valid: 60000\n",
      "# of test: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"train_valid: %s\" % type(train_valid))\n",
    "print(\"test: %s\" % type(test))\n",
    "\n",
    "print(\"# of train_valid: %s\" % len(train_valid))\n",
    "print(\"# of test: %s\" % len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of x0: 784\n",
      "\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411768 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.9686275  0.49803925 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764707 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.882353   0.6745098  0.9921569  0.9490197  0.76470596 0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215688\n",
      " 0.9333334  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9843138  0.3647059  0.32156864\n",
      " 0.32156864 0.21960786 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.8588236  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.77647066 0.7137255\n",
      " 0.9686275  0.9450981  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960788 0.9921569\n",
      " 0.9921569  0.80392164 0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.9921569  0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.9921569  0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.9921569  0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.9450981\n",
      " 0.882353   0.627451   0.42352945 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.94117653 0.9921569\n",
      " 0.9921569  0.4666667  0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.9921569  0.9921569\n",
      " 0.5882353  0.10588236 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.98823535 0.9921569  0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647065 0.9921569  0.97647065 0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.9921569\n",
      " 0.9921569  0.8117648  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980393  0.9921569  0.9921569  0.9921569  0.9803922  0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411766 0.44705886 0.86666673 0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.78823537 0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352942  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.77647066 0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.8588236  0.9921569  0.9921569  0.9921569  0.9921569  0.76470596\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568629 0.6745098  0.8862746  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9568628  0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.9921569\n",
      " 0.9921569  0.9921569  0.8313726  0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x0, y0 = train_valid[0]\n",
    "print(\"size of x0: %s\\n\" % len(x0))\n",
    "\n",
    "print(x0)\n",
    "print(y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "\n",
    "Split dataset so that the ratio of train-set : validation-set : test-set becomes around 60% : 20% : 20%.\n",
    "\n",
    "Ref. https://www.youtube.com/watch?v=M3qpIzy4MQk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = split_dataset_random(train_valid, 50000, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training dataset:  50000\n",
      "# of validation dataset:  10000\n",
      "# of test dataset:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"# of training dataset: \",  len(train))\n",
    "print(\"# of validation dataset: \", len(valid))\n",
    "print(\"# of test dataset: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare iterators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batch_size)\n",
    "valid_iter = iterators.SerialIterator(valid, batch_size, repeat=False, shuffle=False)\n",
    "test_iter  = iterators.SerialIterator(test, batch_size, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Logistic Regression Model\n",
    "\n",
    "Getting familiar with Chainer, let's build a simple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(Chain):\n",
    "    \n",
    "    def __init__(self, n_out = 10):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.fc = L.Linear(None, n_out)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model,  train_iter, valid_iter, max_epoch=10, gpu_id = -1):\n",
    "    if gpu_id >= 0:\n",
    "        model.to_gpu(gpu_id)\n",
    "\n",
    "    optimizer = optimizers.SGD()\n",
    "    optimizer.setup(model)\n",
    "    \n",
    "    updater = training.updater.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "\n",
    "    trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='.out/mnist_result')\n",
    "    \n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.snapshot(filename='snapshot_epoch-{.updater.epoch}'))\n",
    "    trainer.extend(extensions.snapshot_object(model.predictor, filename='model_epoch-{.updater.epoch}'))\n",
    "    trainer.extend(extensions.Evaluator(valid_iter, model, device=gpu_id))\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    trainer.extend(extensions.dump_graph('main/loss'))\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = L.Classifier(model)\n",
    "gpu_id = -1\n",
    "\n",
    "trainer = create_trainer(model, train_iter, valid_iter, max_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.3489      0.704803       0.894117              0.825752                  1.63583       \n",
      "\u001b[J2           0.764448    0.840813       0.668779              0.852848                  3.72292       \n",
      "\u001b[J3           0.620556    0.857492       0.577383              0.865012                  5.74692       \n",
      "\u001b[J4           0.55185     0.867148       0.526616              0.870945                  7.7762        \n",
      "\u001b[J5           0.509772    0.873561       0.493186              0.875494                  9.82244       \n",
      "\u001b[J6           0.481613    0.878105       0.469602              0.878758                  11.8938       \n",
      "\u001b[J7           0.460492    0.882013       0.451425              0.881527                  14.1173       \n",
      "\u001b[J8           0.443764    0.884956       0.437176              0.883999                  16.5765       \n",
      "\u001b[J9           0.430723    0.887228       0.425203              0.887263                  18.5882       \n",
      "\u001b[J10          0.419608    0.888867       0.415581              0.888647                  20.6155       \n",
      "\u001b[J11          0.410795    0.890425       0.407248              0.890229                  22.5651       \n",
      "\u001b[J12          0.402564    0.892463       0.400049              0.892306                  24.599        \n",
      "\u001b[J13          0.395509    0.893642       0.393721              0.893987                  26.5984       \n",
      "\u001b[J14          0.389819    0.894732       0.388535              0.894581                  28.5631       \n",
      "\u001b[J15          0.384112    0.89602        0.383357              0.895965                  30.9328       \n",
      "\u001b[J16          0.379294    0.896875       0.378523              0.897547                  32.9857       \n",
      "\u001b[J17          0.374983    0.897778       0.374452              0.899229                  35.3569       \n",
      "\u001b[J18          0.370631    0.899057       0.37077               0.899525                  37.5287       \n",
      "\u001b[J19          0.367138    0.89972        0.367189              0.900613                  40.0307       \n",
      "\u001b[J20          0.363474    0.900396       0.36434               0.901602                  43.1036       \n",
      "\u001b[J21          0.360698    0.900675       0.361446              0.901998                  45.7917       \n",
      "\u001b[J22          0.357595    0.901462       0.3586                0.902393                  47.9647       \n",
      "\u001b[J23          0.354656    0.902314       0.356321              0.902888                  49.9608       \n",
      "\u001b[J24          0.352467    0.902764       0.353823              0.90358                   51.9704       \n",
      "\u001b[J25          0.349809    0.902913       0.351694              0.904272                  54.017        \n",
      "\u001b[J26          0.34781     0.903692       0.349143              0.904569                  56.0154       \n",
      "\u001b[J27          0.345365    0.904527       0.347153              0.905854                  58.3398       \n",
      "\u001b[J28          0.343336    0.904731       0.345289              0.906349                  60.8234       \n",
      "\u001b[J29          0.341684    0.905531       0.343503              0.905558                  62.9428       \n",
      "\u001b[J30          0.339871    0.905769       0.341934              0.906744                  65.1292       \n",
      "\u001b[J31          0.338033    0.90611        0.340229              0.906744                  67.8918       \n",
      "\u001b[J32          0.336318    0.906571       0.339268              0.907931                  69.8996       \n",
      "\u001b[J33          0.33483     0.906949       0.337373              0.907041                  72.0921       \n",
      "\u001b[J34          0.333334    0.907249       0.336156              0.908129                  74.5858       \n",
      "\u001b[J35          0.331796    0.907472       0.334829              0.908821                  77.1602       \n",
      "\u001b[J36          0.330392    0.907888       0.333587              0.908327                  79.8575       \n",
      "\u001b[J37          0.329905    0.907888       0.332383              0.90892                   82.072        \n",
      "\u001b[J38          0.327218    0.908494       0.331185              0.909316                  84.2989       \n",
      "\u001b[J39          0.32689     0.908548       0.330151              0.909217                  86.5769       \n",
      "\u001b[J40          0.325381    0.909075       0.328925              0.910008                  89.1722       \n",
      "\u001b[J41          0.324467    0.909087       0.327686              0.909711                  91.1921       \n",
      "\u001b[J42          0.322879    0.909787       0.327224              0.910008                  93.3633       \n",
      "\u001b[J43          0.322512    0.910056       0.326019              0.910502                  95.6238       \n",
      "\u001b[J44          0.321526    0.910186       0.324958              0.910898                  98.2878       \n",
      "\u001b[J45          0.319883    0.910965       0.324177              0.911491                  100.981       \n",
      "\u001b[J46          0.319405    0.911058       0.323508              0.911096                  103.347       \n",
      "\u001b[J47          0.318284    0.911345       0.322548              0.911986                  105.511       \n",
      "\u001b[J48          0.317243    0.911518       0.322017              0.91248                   107.64        \n",
      "\u001b[J49          0.316626    0.911585       0.320863              0.911788                  109.702       \n",
      "\u001b[J50          0.31568     0.911885       0.320307              0.912085                  111.721       \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Logistic Regression with test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9152492088607594\n"
     ]
    }
   ],
   "source": [
    "test_evaluator = extensions.Evaluator(test_iter, model, device=gpu_id)\n",
    "results = test_evaluator()\n",
    "print('Test accuracy:', results['main/accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulid Multi Layer Perceptron\n",
    "\n",
    "Let's buld Multi Layer Perceptron model to achive more than 91% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "\n",
    "    def __init__(self, n_mid_units=100, n_out=10):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_mid_units)\n",
    "            self.l2 = L.Linear(None, n_mid_units)\n",
    "            self.l3 = L.Linear(None, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.76843     0.556666       1.03211               0.794798                  3.21304       \n",
      "\u001b[J2           0.712873    0.836057       0.530062              0.86165                   6.80991       \n",
      "\u001b[J3           0.468432    0.877444       0.419703              0.881527                  10.5739       \n",
      "\u001b[J4           0.393125    0.892223       0.369035              0.896855                  14.3409       \n",
      "\u001b[J5           0.356473    0.900116       0.343895              0.900712                  17.9549       \n",
      "\u001b[J6           0.332784    0.905509       0.32355               0.907832                  22.0274       \n",
      "\u001b[J7           0.31558     0.909187       0.309192              0.911491                  25.7009       \n",
      "\u001b[J8           0.302158    0.914203       0.299162              0.914854                  29.53         \n",
      "\u001b[J9           0.290273    0.91696        0.286628              0.919699                  33.2944       \n",
      "\u001b[J10          0.279376    0.919997       0.276967              0.921183                  36.9547       \n",
      "\u001b[J11          0.270495    0.922516       0.270537              0.923259                  40.4871       \n",
      "\u001b[J12          0.261518    0.924772       0.262888              0.925831                  44.1646       \n",
      "\u001b[J13          0.253067    0.92745        0.254169              0.928204                  48.3231       \n",
      "\u001b[J14          0.245552    0.929067       0.249067              0.930083                  52.3052       \n",
      "\u001b[J15          0.238247    0.932085       0.240426              0.931764                  56.1935       \n",
      "\u001b[J16          0.231313    0.934255       0.234351              0.933445                  60.0312       \n",
      "\u001b[J17          0.225139    0.935742       0.229678              0.935028                  64.6878       \n",
      "\u001b[J18          0.218526    0.93794        0.224313              0.935423                  68.601        \n",
      "\u001b[J19          0.21306     0.939583       0.218573              0.938983                  72.6545       \n",
      "\u001b[J20          0.207324    0.941436       0.214936              0.940071                  76.7984       \n",
      "\u001b[J21          0.202309    0.942495       0.209312              0.940763                  80.8764       \n",
      "\u001b[J22          0.197394    0.94379        0.204417              0.941851                  84.7361       \n",
      "\u001b[J23          0.192737    0.945492       0.200477              0.944324                  88.5846       \n",
      "\u001b[J24          0.187788    0.946895       0.19919               0.943928                  92.3408       \n",
      "\u001b[J25          0.183829    0.94833        0.19423               0.946005                  96.2527       \n",
      "\u001b[J26          0.179737    0.949149       0.189794              0.946697                  99.5542       \n",
      "\u001b[J27          0.17532     0.9503         0.18591               0.948081                  103.477       \n",
      "\u001b[J28          0.171718    0.950867       0.184521              0.947983                  107.286       \n",
      "\u001b[J29          0.168016    0.951906       0.181276              0.948576                  111.516       \n",
      "\u001b[J30          0.164546    0.953205       0.17676               0.949763                  115.831       \n",
      "\u001b[J31          0.160941    0.953924       0.173703              0.949466                  120.235       \n",
      "\u001b[J32          0.157681    0.954848       0.171495              0.951345                  124.109       \n",
      "\u001b[J33          0.154391    0.955782       0.170434              0.952037                  127.499       \n",
      "\u001b[J34          0.151316    0.956662       0.165727              0.952235                  130.807       \n",
      "\u001b[J35          0.14848     0.957432       0.163721              0.953224                  134.291       \n",
      "\u001b[J36          0.145236    0.95832        0.163069              0.953817                  137.749       \n",
      "\u001b[J37          0.142697    0.958919       0.158735              0.954707                  141.022       \n",
      "\u001b[J38          0.139927    0.959335       0.157561              0.954608                  144.304       \n",
      "\u001b[J39          0.137142    0.960818       0.154567              0.954312                  147.67        \n",
      "\u001b[J40          0.134981    0.961438       0.153471              0.954905                  150.955       \n",
      "\u001b[J41          0.132392    0.961597       0.151055              0.955696                  154.127       \n",
      "\u001b[J42          0.129587    0.963135       0.14945               0.957377                  157.295       \n",
      "\u001b[J43          0.127344    0.963502       0.146647              0.956685                  160.488       \n",
      "\u001b[J44          0.125354    0.964314       0.145016              0.957872                  163.709       \n",
      "\u001b[J45          0.123231    0.965233       0.143579              0.959059                  166.921       \n",
      "\u001b[J46          0.120874    0.965685       0.141149              0.957674                  170.154       \n",
      "\u001b[J47          0.118937    0.966073       0.141606              0.95896                   173.305       \n",
      "\u001b[J48          0.116667    0.966707       0.138012              0.95985                   176.516       \n",
      "\u001b[J49          0.114805    0.967191       0.136518              0.960938                  179.78        \n",
      "\u001b[J50          0.113026    0.968031       0.135293              0.959355                  183.016       \n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "model = L.Classifier(model)\n",
    "\n",
    "train_iter.reset()\n",
    "valid_iter.reset()\n",
    "test_iter.reset()\n",
    "\n",
    "trainer = create_trainer(model, train_iter, valid_iter, max_epoch=50)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate MLP with test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9636075949367089\n"
     ]
    }
   ],
   "source": [
    "gpu_id = -1\n",
    "\n",
    "test_evaluator = extensions.Evaluator(test_iter, model, device=gpu_id)\n",
    "results = test_evaluator()\n",
    "print('Test accuracy:', results['main/accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Chainer Guides](https://docs.chainer.org/en/stable/guides/index.html)\n",
    "- [Chainer v4 ビギナー向けチュートリアル](https://qiita.com/mitmul/items/1e35fba085eb07a92560#%E5%AD%A6%E7%BF%92%E3%83%AB%E3%83%BC%E3%83%97%E3%82%92%E6%9B%B8%E3%81%84%E3%81%A6%E3%81%BF%E3%82%88%E3%81%86)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
